{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02128189-61b4-4377-ada2-1173e753d77f",
   "metadata": {},
   "source": [
    "# Customizable Design Notebook\n",
    "\n",
    "This notebook contains a scaffold of a simple optimization for a custom state-level property.\n",
    "Please implement your own state-level observable in `observable_fn` and set the value of `target_observable`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024a686-f587-4a0c-8ee6-fd9a4ea8cd02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports\n",
    "\n",
    "We import necessary libraries, e.g. `jax` and `jax-md`.\n",
    "This section can likely remain unchanged, unless you need special libraries for your observable implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3a78ad-2da4-4858-9b2c-7eef4923e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as onp\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "from jax import vmap, jit, value_and_grad, lax, random\n",
    "from jax_md import space, simulate\n",
    "import optax\n",
    "\n",
    "from idp_design.energy_prob import get_energy_fn\n",
    "import idp_design.utils as utils\n",
    "import idp_design.checkpoint as checkpoint\n",
    "import idp_design.observable as observable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd85dca-3953-4415-905a-2b668fd62800",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Constants\n",
    "\n",
    "Here we define constants for the optimization.\n",
    "This includes\n",
    "- the sequence length\n",
    "- hyperparameters for sampling reference states via simulation (e.g. the number of steps)\n",
    "- hyperparameters for optimization (e.g. learning rate, number of iterations).\n",
    "\n",
    "You may wish to change these parameters depending on your optimization, e.g. longer sequences will require longer simulations to sample a representative ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f0fbbb-cfd3-4566-a82e-82d448f5f456",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m res_masses \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mmasses\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Sequence parameters\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "key = random.PRNGKey(0)\n",
    "res_masses = utils.masses\n",
    "\n",
    "# Sequence parameters\n",
    "seq_length = 50\n",
    "\n",
    "bonded_nbrs = jnp.array([(i, i+1) for i in range(seq_length-1)])\n",
    "unbonded_nbrs = list()\n",
    "for pair in itertools.combinations(jnp.arange(seq_length), 2):\n",
    "    unbonded_nbrs.append(pair)\n",
    "unbonded_nbrs = jnp.array(unbonded_nbrs)\n",
    "\n",
    "unbonded_nbrs_set = set([tuple(pr) for pr in onp.array(unbonded_nbrs)])\n",
    "bonded_nbrs_set = set([tuple(pr) for pr in onp.array(bonded_nbrs)])\n",
    "unbonded_nbrs = jnp.array(list(unbonded_nbrs_set - bonded_nbrs_set))\n",
    "\n",
    "# Simulation parameters\n",
    "n_sims = 10\n",
    "n_eq_steps = 10000\n",
    "n_sample_steps = 250000\n",
    "sample_every = 1000\n",
    "assert(n_sample_steps % sample_every == 0)\n",
    "num_points_per_batch = n_sample_steps // sample_every\n",
    "n_ref_states = num_points_per_batch * n_sims\n",
    "\n",
    "kT = 300*utils.kb\n",
    "beta = 1 / kT\n",
    "dt = 0.2\n",
    "gamma = 0.001\n",
    "\n",
    "n_iters = 50\n",
    "lr = 0.1\n",
    "min_neff_factor = 0.90\n",
    "min_n_eff = int(n_ref_states * min_neff_factor)\n",
    "max_approx_iters = 5\n",
    "\n",
    "gumbel_end = 0.01\n",
    "gumbel_start = 1.0\n",
    "gumbel_temps = onp.linspace(gumbel_start, gumbel_end, n_iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8315cd-144b-496b-ae1a-9083bf4135da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Helper Functions\n",
    "\n",
    "Here we define helper functions\n",
    "\n",
    "First, we define misc. helper functions such as\n",
    "- Normalization of the logits to a probabilistic sequence representation. Note that we optimize a `(n, 20)` set of logits that can be deterministically mapped to a probabilistic sequence.\n",
    "- An energy function that computes the expected energy of the probabilistic sequence\n",
    "\n",
    "We then define helper functions specific to simulation via `jax-md` based on the hyperparameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a3d45-6364-4a3a-9f50-4c8b82b0587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(logits, temp, norm_key):\n",
    "    gumbel_weights = jax.random.gumbel(norm_key, logits.shape)\n",
    "    pseq = jax.nn.softmax(logits / temp)\n",
    "\n",
    "    return pseq\n",
    "\n",
    "displacement_fn, shift_fn = space.free()\n",
    "\n",
    "subterms_fn, energy_fn = get_energy_fn(bonded_nbrs, unbonded_nbrs, displacement_fn)\n",
    "energy_fn = jit(energy_fn)\n",
    "mapped_energy_fn = vmap(energy_fn, (0, None)) # To evaluate a set of states for a given pseq\n",
    "\n",
    "checkpoint_every = 50\n",
    "scan = checkpoint.get_scan(checkpoint_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2637d-b104-482c-878b-226fcca4176c",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e3beb-bde0-4ebb-8be1-70f6b549e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def eq_fn(eq_key, R, pseq, mass):\n",
    "    init_fn, step_fn = simulate.nvt_langevin(energy_fn, shift_fn, dt, kT, gamma)\n",
    "    init_state = init_fn(eq_key, R, pseq=pseq, mass=mass)\n",
    "    def fori_step_fn(t, state):\n",
    "        return step_fn(state, pseq=pseq)\n",
    "    fori_step_fn = jit(fori_step_fn)\n",
    "\n",
    "    eq_state = lax.fori_loop(0, n_eq_steps, fori_step_fn, init_state)\n",
    "    return eq_state.position\n",
    "\n",
    "@jit\n",
    "def sample_fn(sample_key, R_eq, pseq, mass):\n",
    "    init_fn, step_fn = simulate.nvt_langevin(energy_fn, shift_fn, dt, kT, gamma)\n",
    "    init_state = init_fn(sample_key, R_eq, pseq=pseq, mass=mass)\n",
    "\n",
    "    def fori_step_fn(t, state):\n",
    "        return step_fn(state, pseq=pseq)\n",
    "    fori_step_fn = jit(fori_step_fn)\n",
    "\n",
    "    @jit\n",
    "    def scan_fn(state, step):\n",
    "        state = lax.fori_loop(0, sample_every, fori_step_fn, state)\n",
    "        return state, state.position\n",
    "\n",
    "    _, traj = lax.scan(scan_fn, init_state, jnp.arange(num_points_per_batch))\n",
    "    return traj\n",
    "\n",
    "@jit\n",
    "def batch_sim(ref_key, R, pseq, mass):\n",
    "\n",
    "    ref_key, eq_key = random.split(ref_key)\n",
    "    eq_keys = random.split(eq_key, n_sims)\n",
    "    eq_states = vmap(eq_fn, (0, None, None, None))(eq_keys, R, pseq, mass)\n",
    "\n",
    "    sample_keys = random.split(ref_key, n_sims)\n",
    "    sample_trajs = vmap(sample_fn, (0, 0, None, None))(sample_keys, eq_states, pseq, mass)\n",
    "\n",
    "    sample_traj = sample_trajs.reshape(-1, seq_length, 3)\n",
    "    return sample_traj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97343352-2a33-458c-a68a-43f0ceb5ae83",
   "metadata": {},
   "source": [
    "## Define Custom Observable\n",
    "\n",
    "Here, you may define a **custom observable** as the target for optimization.\n",
    "Observables are defined at the state-level and you may optimize an arbitrary\n",
    "(continuous and differentiable) function of the expected value of the observable\n",
    "across the entire ensemble. This notebook assumes that you only wish to optimize\n",
    "a simple root mean square error (RMSE) from a target value. Note that certain\n",
    "observables may require enhanced sampling to obtain a representative distribution,\n",
    "which we do not implement here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4a291-ac7c-47d5-bb42-db4537306038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def observable_fn(R):\n",
    "    \"\"\"\n",
    "    This defines a state-level observable for which you would like to \n",
    "    optimize a sequence with a target expected value. `R` is an `(n, 3)`\n",
    "    JAX array denoting the positions of the `n` particles. \n",
    "    \n",
    "    For example, to optimize the end-to-end distance, this function would \n",
    "    return:\n",
    "    \n",
    "    `jnp.linalg.norm(displacement_fn(R[0], R[-1]))`\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "target_observable = FIXME # The target value of the observable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b9df0-6347-4064-a21b-316c625192a6",
   "metadata": {},
   "source": [
    "## Define Function for Getting Reference States\n",
    "\n",
    "Given helper functions for simulation via `jax-md` and our observable, \n",
    "we define a function that samples reference states for use in differentiable\n",
    "trajectory reweighting (DiffTRe). This includes running a simulation, sampling\n",
    "states with some periodicity (defined as `sample_every` above), and returning\n",
    "- the sampled states\n",
    "- the calculated observable of each reference state\n",
    "- the calculated energy of each reference state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871208ab-56df-4805-b378-73ddc9b53e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_states(params, i, R, iter_key, temp):\n",
    "    curr_logits = params['logits']\n",
    "    iter_key, norm_key = random.split(iter_key)\n",
    "    curr_pseq = normalize(curr_logits, temp, norm_key)\n",
    "\n",
    "    iter_dir = ref_traj_dir / f\"iter{i}\"\n",
    "    iter_dir.mkdir(parents=False, exist_ok=False)\n",
    "\n",
    "    curr_mass = utils.get_pseq_mass(curr_pseq, res_masses=res_masses)\n",
    "\n",
    "    iter_key, batch_key = random.split(iter_key)\n",
    "    start = time.time()\n",
    "    sample_traj = batch_sim(batch_key, R, curr_pseq, curr_mass)\n",
    "    end = time.time()\n",
    "    print(f\"- Batched simulation took {end - start} seconds\")\n",
    "    sample_traj = utils.tree_stack(sample_traj)\n",
    "\n",
    "    sample_observables = vmap(observable_fn(sample_traj)\n",
    "    mean_observable = onp.mean(sample_observables)\n",
    "\n",
    "    sample_energies = mapped_energy_fn(sample_traj, curr_pseq)\n",
    "\n",
    "    return sample_traj, sample_energies, jnp.array(sample_observables), mean_observable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c84e788-ff13-473d-9616-823a1c923177",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Define Loss Function\n",
    "\n",
    "The second required component for DiffTRe is a loss function that takes as input a reference ensemble and\n",
    "returns the loss as a function of the computed expected value of the observable by reweighting with\n",
    "respect to the current parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd224541-f3e0-48be-8b9b-60de9c8ee1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, ref_states, ref_energies, ref_observables, temp, loss_key):\n",
    "    logits = params['logits']\n",
    "    loss_key, norm_key = random.split(loss_key)\n",
    "    pseq = normalize(logits, temp, norm_key)\n",
    "\n",
    "    energy_scan_fn = lambda state, ts: (None, energy_fn(ts, pseq=pseq))\n",
    "    _, new_energies = scan(energy_scan_fn, None, ref_states)\n",
    "\n",
    "    weights, n_eff = utils.compute_weights(ref_energies, new_energies, beta)\n",
    "    weighted_observables = weights * ref_observables # element-wise multiplication\n",
    "    expected_observable = jnp.sum(weighted_observables)\n",
    "\n",
    "\n",
    "    mse = (expected_observable - target_observable)**2\n",
    "    rmse = jnp.sqrt(mse)\n",
    "    loss = rmse\n",
    "\n",
    "    return loss, (n_eff, expected_observable, pseq)\n",
    "grad_fn = value_and_grad(loss_fn, has_aux=True)\n",
    "grad_fn = jit(grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a6db5-989c-469b-bb39-446e0da1778d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup Optimization\n",
    "\n",
    "Finally, we set up the final pieces of our optimization:\n",
    "- the initialized `(n, 20)` logits (that can be deterministically mapped to a probabilistic sequence)\n",
    "- an optimizer\n",
    "- an initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5c8ac-c18a-4421-b1f6-535275f4da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_logits = onp.full((seq_length, 20), 100.0)\n",
    "init_logits = jnp.array(init_logits, dtype=jnp.float64)\n",
    "\n",
    "# Setup the optimization\n",
    "params = {\"logits\": init_logits}\n",
    "optimizer = optax.adam(learning_rate=lr)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "R_init = list()\n",
    "init_spring_r0 = utils.spring_r0\n",
    "for i in range(seq_length):\n",
    "    R_init.append([out_box_size/2, out_box_size/2, out_box_size/2+init_spring_r0*i])\n",
    "R_init = jnp.array(R_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0fd8cb-6ff5-4803-b059-4af3fdaa4d7d",
   "metadata": {},
   "source": [
    "## Generate Initial Reference States\n",
    "\n",
    "Before we can do our first gradient update, we have to sample an initial set of reference states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564d860b-7f14-4a6a-aa02-b1e0c3f2cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, iter_key = random.split(key)\n",
    "ref_states, ref_energies, ref_observables, mean_observable = get_ref_states(params, 0, R_init, iter_key, temp=gumbel_temps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1329fa7-4784-43c3-afb5-eeed4d5fb186",
   "metadata": {},
   "source": [
    "## Optimize\n",
    "\n",
    "We can then perform gradient descent iteratively, resampling reference states when necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b68735-1c80-41b1-a87e-df2f3c553530",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_resample_iters = 0\n",
    "for i in range(n_iters):\n",
    "    print(f\"\\nIteration {i}:\")\n",
    "    key, loss_key = random.split(key)\n",
    "    (loss, aux), grads = grad_fn(params, ref_states, ref_energies, ref_observables, gumbel_temps[i], loss_key)\n",
    "    n_eff = aux[0]\n",
    "    num_resample_iters += 1\n",
    "\n",
    "    if n_eff < min_n_eff or num_resample_iters > max_approx_iters:\n",
    "        print(f\"- N_eff was {n_eff}... resampling reference states...\")\n",
    "        num_resample_iters = 0\n",
    "\n",
    "        key, iter_key = random.split(key)\n",
    "        ref_states, ref_energies, ref_observables, mean_rg = get_ref_states(\n",
    "            params, i, utils.recenter(ref_states[-1], out_box_size), iter_key, gumbel_temps[i]\n",
    "        )\n",
    "\n",
    "        (loss, aux), grads = grad_fn(params, ref_states, ref_energies, ref_observables, gumbel_temps[i], loss_key)\n",
    "    (n_eff, expected_observable, pseq) = aux\n",
    "\n",
    "\n",
    "    max_residues = jnp.argmax(pseq, axis=1)\n",
    "    argmax_seq = ''.join([utils.RES_ALPHA[res_idx] for res_idx in max_residues])\n",
    "    print(f\"- Argmax seq: {argmax_seq}\")\n",
    "    \n",
    "    print(f\"- Loss: {loss}\")\n",
    "    print(f\"- Current value: {expected_observable}\")\n",
    "\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
